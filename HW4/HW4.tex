\documentclass[11pt]{article}
\setlength{\topmargin}{-1in}
\setlength{\textheight}{10in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\textwidth}{7in}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{array}
\usepackage{bm}
\usepackage{cancel}
\usepackage{cite}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{units}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{ mathrsfs }
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{setspace}

\pagenumbering{gobble}

\usepackage{floatrow}
\floatsetup[figure]{capposition=top}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}

\usepackage{color}
\lstset{language=R,basicstyle=\ttfamily,breaklines=true,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{magenta}\ttfamily,
                showstringspaces=false,
                }

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand*\mb{\mathbf}
\newcommand*\reals{\mathbb{R}}
\newcommand*\complex{\mathbb{C}}
\newcommand*\naturals{\mathbb{N}}
\newcommand*\nats{\naturals}
\newcommand*\integers{\mathbb{Z}}
\newcommand*\rationals{\mathbb{Q}}
\newcommand*\irrationals{\mathbb{J}}
\newcommand*\pd{\partial}
\newcommand*\htab{\hspace{4 mm}}
\newcommand*\vtab{\vspace{0.5 in}}
\newcommand*\lsent{\mathcal{L}}
\newcommand*\conj{\overline}
\newcommand*\union{\cup}
\newcommand*\intersect{\cap}
\newcommand*\cl{\cancel}
\newcommand*\ANS{\text{ANS}}
\newcommand*\As{\text{As}}
\newcommand*\then{\rightarrow}
\newcommand*\elim{\text{E}}
\newcommand*\intro{\text{I}}
\newcommand*\absurd{\curlywedge}
\newcommand*\NK{\vdash_{\text{NK}}}
\newcommand*\derivation{\begin{tabular} { >{$}l<{$}  >{$}c<{$}  >{$}l<{$}  >{$}r<{$} }}
\newcommand*\interp{\mathcal{I}}
\newcommand*\ba{\[ \begin{aligned}}
\newcommand*\ea{\end{aligned} \]}
\newcommand*\C{\mathcal{C}}
\newcommand*\card[1]{\text{card}\left(#1\right)}
\newcommand*\D{\mathscr{D}}
\newcommand*\df{=_{\text{def}}}
\newcommand*\eps{\epsilon}
\newcommand*\enum{\begin{enumerate}[label=(\alph*)]}
\newcommand*\enumend{\end{enumerate}}
\newcommand*\E[1]{\mathsf{E}\left[#1\right]}
\newcommand*\Esub[2]{\mathsf{E}_{#1}\left[#2\right]}
\newcommand*\Var[1]{\mathsf{Var}\left(#1\right)}
\newcommand*\Cov[1]{\;\text{Cov}\left[#1\right]}
\newcommand*\iid{\overset{\text{iid}}{\sim}}
\newcommand*\Exp[1][\lambda]{\text{Exp}(\text{rate}=#1)}
\newcommand*\ind[1]{\mathbf{1}\left(#1\right)}
\newcommand*\set[1]{\left\{#1\right\}}
\newcommand*\estim[1]{\widehat{#1}}
\newcommand*\der{\text{d}}
\newcommand*\deriv[2]{\frac{\der #1}{\der #2}}
\newcommand*\norm[1]{\left\|#1\right\|}
\newcommand*\dist[2]{\;\text{dist}\left(#1, #2\right)}
\newcommand*\interior{\text{int}\;}
\newcommand*\exterior{\text{ext}\;}
\newcommand*\boundary{\text{bd}\;}
\newcommand*\lh{\overset{\text{L'H}}{=}}
\newcommand*\bA{\mathbf{A}}
\newcommand*\bb{\mathbf{b}}
\newcommand*\bB{\mathbf{B}}
\newcommand*\bc{\mathbf{c}}
\newcommand*\be{\mathbf{e}}
\newcommand*\bh{\mathbf{h}}
\newcommand*\bI{\mathbf{I}}
\newcommand*\bL{\mathbf{L}}
\newcommand*\bo{\mathbf{o}}
\newcommand*\br{\mathbf{r}}
\newcommand*\bs{\mathbf{s}}
\newcommand*\bS{\mathbf{S}}
\newcommand*\bt{\mathbf{t}}
\newcommand*\bu{\mathbf{u}}
\newcommand*\bv{\mathbf{v}}
\newcommand*\bx{\mathbf{x}}
\newcommand*\bw{\mathbf{w}}
\newcommand*\bW{\mathbf{W}}
\newcommand*\bX{\mathbf{X}}
\newcommand*\by{\mathbf{y}}
\newcommand*\bY{\mathbf{Y}}
\newcommand*\bZ{\mathbf{Z}}
\newcommand*\bz{\mathbf{z}}
\newcommand*\bp{\mathbf{p}}
\newcommand*\bzero{\mathbf{0}}
\newcommand*\bone{\mathbf{1}}
\newcommand*\balpha{\boldsymbol{\alpha}}
\newcommand*\bbeta{\boldsymbol{\beta}}
\newcommand*\bgamma{\boldsymbol{\gamma}}
\newcommand*\beps{\boldsymbol{\varepsilon}}
\newcommand*\btheta{\boldsymbol{\theta}}
\newcommand*\bTheta{\boldsymbol{\Theta}}
\newcommand*\bmu{\boldsymbol{\mu}}
\newcommand*\bsigma{\boldsymbol{\sigma}}
\newcommand*\bSigma{\boldsymbol{\Sigma}}
\newcommand*\bOmega{\boldsymbol{\Omega}}
\newcommand\Psub[2]{\mathsf{P}_{#1}\left(#2\right)}
\newcommand\Vsub[2]{\mathsf{Var}_{#1}\left(#2\right)}
\newcommand\e{\operatorname{e}}
\newcommand\prox{\operatorname{prox}}
\newcommand\T{\mathsf{T}}

\newread\tmp
\newcommand\getcount{
	\openin\tmp=knot_count.txt
	\read\tmp to \knots
	\closein\tmp
	
	\openin\tmp=span.txt
	\read\tmp to \spanval
	\closein\tmp
	
	\openin\tmp=span_g.txt
	\read\tmp to \spantwo
	\closein\tmp
	
	\openin\tmp=span_g_2.txt
	\read\tmp to \spanthree
	\closein\tmp
}


\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\renewcommand\;{\,}
\renewcommand\epsilon{\varepsilon}
\renewcommand\rho{\varrho}
\renewcommand\phi{\varphi}
\renewcommand\mod{\hspace{0.2em} \textbf{mod}\hspace{0.2em}}
\renewcommand\Pr[1]{ \mathsf{Pr}\left(#1\right) }

\lstset{breaklines=true,
        numbersep=5pt,
        xleftmargin=.25in,
        xrightmargin=.25in}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\vspace{-1in}
\begin{document}
\title{STAT 570: Homework 4}
\author{Branden Olson}
\date{}
\maketitle

\section*{Problem 1}
\enum
\item
Our likelihood is of the form
\ba
L(\lambda| \by) & \propto p(\by| \lambda) \\
	& = \prod_{i=1}^n \left[\lambda \exp\left(-\lambda y_i\right)\right] \\
	& = \lambda^n \exp\left(-\lambda \sum_{i=1}^n y_i \right)
\ea
whereas our prior density is
\ba
\pi(\lambda) & = \frac{b^a}{\Gamma(a)} \lambda^{a - 1} \exp\left(-b \lambda\right).
\ea
Thus, our posterior is
\ba
p(\lambda | \by) & \propto L(\lambda | \by) \pi(\lambda) \\
	& = \lambda^n \exp\left(-\lambda \sum_{i=1}^n y_i \right)
		\frac{b^a}{\Gamma(a)} \lambda^{a - 1} \exp\left(-b \lambda\right) \\
	& \propto \lambda^{n + a - 1} 
		\exp\left(-\lambda\left[ b + \sum_{i=1}^n y_i \right] \right)
\ea
which is the kernel of a Gamma$\left(n + a, b + \sum_{i=1}^n y_i\right)$ density.
\item
Here, we want $a$ and $b$ such that
\ba
0.9 & = \Psub{\lambda}{ 0.1 \le \lambda \le 1 | a, b} \\
	& = \Psub{\lambda}{ \lambda \le 1 | a, b} - 
		\Psub{\lambda}{\lambda \le 0.1 | a, b } \\
	& = F_{\lambda | a, b}(1) - F_{\lambda | a, b}(0.1)
\ea 
where $F_{\lambda | a, b}(t)$ is the cdf of a Gamma($a, b$). In particular, since $F$ is a continuous, strictly increasing function, we are warranted in breaking the above equation into two sub-equations, for example,
\ba
0.05 & = \Psub{\lambda}{\lambda \le 0.1} \\
0.05 & = \Psub{\lambda}{\lambda > 1}
\ea
which yields the nonlinear system of equations 
\ba
\begin{pmatrix}
0.05 - F_{\lambda|a, b}(0.1) \\
0.95 - F_{\lambda|a, b}(1) \\
\end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\ea
This system can be easily solved with the \texttt{nleqslv} package in R (the source can be found in Appendix A). Running our code yields
\ba
\boxed{
a \approx 2.44 \htab b \approx 0.184
}
\ea

\item
Below is the table of posterior means and standard deviations.

\input{mean_sd.tex}

Here are the histograms:

\includegraphics[width=\linewidth]{histograms.pdf}

Next, note that if
 $\lambda | \by \sim \text{Gamma}\left(n + a, b + \sum_{i=1}^n y_i\right)$
we have that
\ba
\left(\frac{1}{\lambda} \right) \bigg| \; \by \sim \text{Inv-Gamma}\left(n + a, 
 	\frac{1}{b + \sum_{i=1}^n y_i} \right).
\ea
\includegraphics[width=\linewidth]{inv_histograms.pdf}
\item
First, we note that, for left-probability $l$ and right-probability $r$,
\ba
0.9 & = \Psub{\eta}{ \ell \le \eta \le r }  \\
	& = \Psub{\eta}{ \ell \le \text{LogNormal}(\mu_\eta, \sigma_\eta) \le r } \\
	& = \Psub{\eta}{ \ell \le \exp(\mu_\eta + \sigma_\eta Z) \le r },
		\htab Z \sim \mathcal{N}(0, 1) \\
	& = \Psub{\eta}{ \log(\ell) \le \mu_\eta + \sigma_\eta Z \le \log(r) } \\
	& = \Psub{\eta}{ \frac{\log(l) - \mu_\eta}{\sigma_\eta}
			\le \mathcal{N}(0, 1) \le
			\frac{\log(r) - \mu_\eta}{\sigma_\eta} } \\
	& = \Phi\left( \frac{\log(r) - \mu_\eta}{\sigma_\eta} \right)
		- \Phi\left( \frac{\log(\ell) - \mu_\eta}{\sigma_\eta} \right)
\ea
which we can deconstruct into the set of equations
\ba
\begin{pmatrix}
0.05 - \Phi\left( \frac{\log(\ell) - \mu_\eta}{\sigma_\eta} \right) \\
0.95 - \Phi\left( \frac{\log(r) - \mu_\eta}{\sigma_\eta} \right)
\end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\ea
In particular, for $\eta$, we have
\ba
\begin{pmatrix}
0.05 - \Phi\left( \frac{\log(0.5) - \mu_\eta}{\sigma_\eta} \right) \\
0.95 - \Phi\left( \frac{\log(30) - \mu_\eta}{\sigma_\eta} \right)
\end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}
\ea
and for $\alpha$,
\ba
\begin{pmatrix}
0.05 - \Phi\left( \frac{0 - \mu_\eta}{\sigma_\eta} \right) \\
0.95 - \Phi\left( \frac{\log(4) - \mu_\eta}{\sigma_\eta} \right)
\end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}
\ea
Solving this with \texttt{nleqslv} yields
\ba
\boxed{
\mu_\eta \approx 1.35, \htab \sigma_\eta \approx 1.25
}
\ea
for $\eta$, and 
\ba
\boxed{
\mu_\alpha \approx 0.693, \htab  \sigma_\alpha \approx 0.421
}
\ea
for $\alpha$.
\item
Here we show the tables of medians and (0.05, 0.95) quantiles, the latter of which together construct 95\% credible intervals. 
\input{table1.tex}
\input{table2.tex}
\input{table3.tex}
\input{table4.tex}

Moreover, we include plots of the marginal histograms as well as scatter plots of samples from the joint posterior $p(\eta, \alpha | \by)$, for each of the four lengths in the \texttt{stress} dataset.
\begin{center}
\includegraphics[width=\linewidth]{pairs1.pdf}
\includegraphics[width=\linewidth]{pairs2.pdf}
\includegraphics[width=\linewidth]{pairs3.pdf}
\includegraphics[width=\linewidth]{pairs4.pdf}
\end{center}
\enumend

\newpage
\section*{Problem 2}
\enum
\item
We note that
\ba
\E{ Y_k | X_k } = n \frac{X_k}{N} \\
\implies X_k = \frac{N \E{Y_k | X_k}}{n}
\ea
The most immediate estimate of $\E{Y_k | X_k}$ is simply the observed $Y_k$, yielding
\ba
\estim{X_k} = \frac{N Y_k}{n}
\ea
with variance
\ba
\Var{\estim{X_k}} & = \frac{N^2}{n^2} \Var{Y_k} \\
	& = \frac{N^2}{n^2} \cdot
		n \frac{X_k}{N} \left(1 - \frac{X_k}{N}\right) \frac{N - n}{N - 1} \\
	& = \frac{N(N -n)}{n(N - 1)} X_k \left(1 - \frac{X_k}{N}\right)
\ea
\item
Assume that the likelihood follows the multinomial distribution in (3) and the prior follows a Dirichlet distribution in (4). 
\ba
p(p_1, \dotsc, p_K | x_1, \dotsc, x_K)
	& \propto L(\bx | \bp) \pi(\bp) \\
	& \propto \prod_{i=1}^K p_k^{x_k} \prod_{i=1}^K p_k^{\alpha_k - 1} \\
	& = \prod_{i=1}^K p_k^{x_k + \alpha_k - 1}
\ea
which is exactly the kernel of a Dirichlet with parameters
$x_k + \alpha_k: k = 1, \dotsc, K$.
\item
Here we have
\ba
p(\tilde \bx) & = \int p_{\bX | \bp}(\tilde \bx | \bp)
	\pi(\bp) \; \der \bp \\
	& = \int \frac{N!}{\prod_{k=1}^K x_k!} \prod_{k=1}^K p_k^{x_k}
		\cdot \frac{\Gamma(\alpha_+)}{\prod_{k=1}^K \Gamma(\alpha_k)}
		\prod_{k=1}^K p_k^{\alpha_k - 1}
		\; \der \bp \\
	& = \frac{N! \Gamma(\alpha_+)}{ \prod_k x_k! \prod_k \Gamma(\alpha_k)}
		\int \prod_{k=1}^K p_k^{x_k + \alpha_k - 1} \; \der \bp \\
	& = \frac{N! \Gamma(\alpha_+)}{ \prod_k x_k! \Gamma(\alpha_k)}
		\frac{ \prod_{k=1}^K \Gamma(\alpha_k + x_k)}
			{ \Gamma(( \alpha + x)_+ ) } \\
	& = \frac{N! \Gamma(\alpha_+)}{ \prod_k x_k! \Gamma(\alpha_k)}
		\frac{ \prod_{k=1}^K \Gamma(\alpha_k + x_k)}
			{ \Gamma(\alpha_k + N ) } \\
\ea
\item
We have 
\ba
\E{X_k} & = \Esub{\bp}{ \Esub{\bX | \bp}{ X_k | \bp } } \\
	& = \Esub{\bp}{ N p_k } \htab \text{since } \bX \sim \text{Mult}_k(N, \bp) \\
	& = N \Esub{\bp}{ p_k } \\
	& = \boxed{ \frac{N \alpha_k}{\alpha_+} }
\ea
and
\ba
\Var{X_k} & = \Vsub{\bp}{ \Esub{\bX | \bp}{ X_k | \bp} } +
	\Esub{\bp}{ \Vsub{\bX | \bp}{ X_k | \bp } } \\
	& = \Vsub{\bp}{ N p_k } +
	\Esub{\bp}{ N p_k(1 - p_k) } \\
	& = N^2 \cdot \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) } +
		N \Esub{\bp}{ p_k - p_k^2 } \\
	& = N^2  \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) } +
		N \left[ \frac{\alpha_k}{\alpha_+} - \left( \Vsub{\bp}{p_k} +
			\Esub{\bp}{p_k}^2 \right) \right] \\
	& = N \left\{  N \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) } +
		 \left[ \frac{\alpha_k}{\alpha_+} 
		 -  \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) }
		-	\left( \frac{\alpha_k}{\alpha_+}\right)^2  \right] \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left\{
		\frac{N(\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)}
		+ 1 - \frac{\alpha_+ - \alpha_k}{\alpha_+(\alpha_+ + 1)}
		- \frac{\alpha_k}{\alpha_+} \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left\{
		\frac{(N - 1) (\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)}
		+ 1 - \frac{\alpha_k}{\alpha_+} \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left\{ \frac{1}{1 - \frac{\alpha_k}{\alpha_+}}
		\frac{(N - 1) (\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)} + 1
		 \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left\{ \frac{\alpha_+}{\alpha_+ - \alpha_k}
		\frac{(N - 1) (\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)} + 1
		 \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left\{ 
		\frac{N - 1}{\alpha_+ + 1} + \frac{\alpha_+ + 1}{\alpha_+ + 1}
		 \right\} \\
	& = \boxed{ \frac{N \alpha_k}{\alpha_+} 
		\left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left[
		\frac{N + \alpha_+ } {\alpha_+ + 1}
		 \right] }
\ea

\enumend
 
\newpage
\section*{Appendix A: Problem 1 Source}
\lstinputlisting{P1.R}
\end{document}



























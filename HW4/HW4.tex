\documentclass[11pt]{article}
\setlength{\topmargin}{-1in}
\setlength{\textheight}{10in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\textwidth}{7in}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{array}
\usepackage{bm}
\usepackage{cancel}
\usepackage{cite}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{units}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{ mathrsfs }
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{setspace}

\pagenumbering{gobble}

\usepackage{floatrow}
\floatsetup[figure]{capposition=top}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}

\usepackage{color}
\lstset{language=R,basicstyle=\ttfamily,breaklines=true,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{magenta}\ttfamily,
                showstringspaces=false,
                }

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand*\mb{\mathbf}
\newcommand*\reals{\mathbb{R}}
\newcommand*\complex{\mathbb{C}}
\newcommand*\naturals{\mathbb{N}}
\newcommand*\nats{\naturals}
\newcommand*\integers{\mathbb{Z}}
\newcommand*\rationals{\mathbb{Q}}
\newcommand*\irrationals{\mathbb{J}}
\newcommand*\pd{\partial}
\newcommand*\htab{\hspace{4 mm}}
\newcommand*\vtab{\vspace{0.5 in}}
\newcommand*\lsent{\mathcal{L}}
\newcommand*\conj{\overline}
\newcommand*\union{\cup}
\newcommand*\intersect{\cap}
\newcommand*\cl{\cancel}
\newcommand*\ANS{\text{ANS}}
\newcommand*\As{\text{As}}
\newcommand*\then{\rightarrow}
\newcommand*\elim{\text{E}}
\newcommand*\intro{\text{I}}
\newcommand*\absurd{\curlywedge}
\newcommand*\NK{\vdash_{\text{NK}}}
\newcommand*\derivation{\begin{tabular} { >{$}l<{$}  >{$}c<{$}  >{$}l<{$}  >{$}r<{$} }}
\newcommand*\interp{\mathcal{I}}
\newcommand*\ba{\[ \begin{aligned}}
\newcommand*\ea{\end{aligned} \]}
\newcommand*\C{\mathcal{C}}
\newcommand*\card[1]{\text{card}\left(#1\right)}
\newcommand*\D{\mathscr{D}}
\newcommand*\df{=_{\text{def}}}
\newcommand*\eps{\epsilon}
\newcommand*\enum{\begin{enumerate}[label=(\alph*)]}
\newcommand*\enumend{\end{enumerate}}
\newcommand*\E[1]{\mathsf{E}\left[#1\right]}
\newcommand*\Esub[2]{\mathsf{E}_{#1}\left[#2\right]}
\newcommand*\Var[1]{\mathsf{Var}\left(#1\right)}
\newcommand*\Cov[1]{\;\text{Cov}\left[#1\right]}
\newcommand*\iid{\overset{\text{iid}}{\sim}}
\newcommand*\Exp[1][\lambda]{\text{Exp}(\text{rate}=#1)}
\newcommand*\ind[1]{\mathbf{1}\left(#1\right)}
\newcommand*\set[1]{\left\{#1\right\}}
\newcommand*\estim[1]{\widehat{#1}}
\newcommand*\der{\text{d}}
\newcommand*\deriv[2]{\frac{\der #1}{\der #2}}
\newcommand*\norm[1]{\left\|#1\right\|}
\newcommand*\dist[2]{\;\text{dist}\left(#1, #2\right)}
\newcommand*\interior{\text{int}\;}
\newcommand*\exterior{\text{ext}\;}
\newcommand*\boundary{\text{bd}\;}
\newcommand*\lh{\overset{\text{L'H}}{=}}
\newcommand*\bA{\mathbf{A}}
\newcommand*\bb{\mathbf{b}}
\newcommand*\bB{\mathbf{B}}
\newcommand*\bc{\mathbf{c}}
\newcommand*\be{\mathbf{e}}
\newcommand*\bh{\mathbf{h}}
\newcommand*\bI{\mathbf{I}}
\newcommand*\bL{\mathbf{L}}
\newcommand*\bo{\mathbf{o}}
\newcommand*\br{\mathbf{r}}
\newcommand*\bs{\mathbf{s}}
\newcommand*\bS{\mathbf{S}}
\newcommand*\bt{\mathbf{t}}
\newcommand*\bu{\mathbf{u}}
\newcommand*\bv{\mathbf{v}}
\newcommand*\bx{\mathbf{x}}
\newcommand*\bw{\mathbf{w}}
\newcommand*\bW{\mathbf{W}}
\newcommand*\bX{\mathbf{X}}
\newcommand*\by{\mathbf{y}}
\newcommand*\bY{\mathbf{Y}}
\newcommand*\bZ{\mathbf{Z}}
\newcommand*\bz{\mathbf{z}}
\newcommand*\bp{\mathbf{p}}
\newcommand*\bzero{\mathbf{0}}
\newcommand*\bone{\mathbf{1}}
\newcommand*\balpha{\boldsymbol{\alpha}}
\newcommand*\bbeta{\boldsymbol{\beta}}
\newcommand*\bgamma{\boldsymbol{\gamma}}
\newcommand*\beps{\boldsymbol{\varepsilon}}
\newcommand*\btheta{\boldsymbol{\theta}}
\newcommand*\bTheta{\boldsymbol{\Theta}}
\newcommand*\bmu{\boldsymbol{\mu}}
\newcommand*\bsigma{\boldsymbol{\sigma}}
\newcommand*\bSigma{\boldsymbol{\Sigma}}
\newcommand*\bOmega{\boldsymbol{\Omega}}
\newcommand\Psub[2]{\mathsf{P}_{#1}\left(#2\right)}
\newcommand\Vsub[2]{\mathsf{Var}_{#1}\left(#2\right)}
\newcommand\e{\operatorname{e}}
\newcommand\prox{\operatorname{prox}}
\newcommand\T{\mathsf{T}}

\newread\tmp
\newcommand\getcount{
	\openin\tmp=knot_count.txt
	\read\tmp to \knots
	\closein\tmp
	
	\openin\tmp=span.txt
	\read\tmp to \spanval
	\closein\tmp
	
	\openin\tmp=span_g.txt
	\read\tmp to \spantwo
	\closein\tmp
	
	\openin\tmp=span_g_2.txt
	\read\tmp to \spanthree
	\closein\tmp
}


\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\renewcommand\;{\,}
\renewcommand\epsilon{\varepsilon}
\renewcommand\rho{\varrho}
\renewcommand\phi{\varphi}
\renewcommand\mod{\hspace{0.2em} \textbf{mod}\hspace{0.2em}}
\renewcommand\Pr[1]{ \mathsf{Pr}\left(#1\right) }

\lstset{breaklines=true,
        numbersep=5pt,
        xleftmargin=.25in,
        xrightmargin=.25in}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\vspace{-1in}
\begin{document}
\title{STAT 570: Homework 4}
\author{Branden Olson}
\date{}
\maketitle

\section*{Problem 1}
\enum
\item
Our likelihood is of the form
\ba
L(\lambda| \by) & \propto p(\by| \lambda) \\
	& = \prod_{i=1}^n \left[\lambda \exp\left(-\lambda y_i\right)\right] \\
	& = \lambda^n \exp\left(-\lambda \sum_{i=1}^n y_i \right)
\ea
whereas our prior density is
\ba
\pi(\lambda) & = \frac{b^a}{\Gamma(a)} \lambda^{a - 1} \exp\left(-b \lambda\right).
\ea
Thus, our posterior is
\ba
p(\lambda | \by) & \propto L(\lambda | \by) \pi(\lambda) \\
	& = \lambda^n \exp\left(-\lambda \sum_{i=1}^n y_i \right)
		\frac{b^a}{\Gamma(a)} \lambda^{a - 1} \exp\left(-b \lambda\right) \\
	& \propto \lambda^{n + a - 1} 
		\exp\left(-\lambda\left[ b + \sum_{i=1}^n y_i \right] \right)
\ea
which is the kernel of a Gamma$\left(n + a, b + \sum_{i=1}^n y_i\right)$ density.
\item
Here, we want $a$ and $b$ such that
\ba
0.9 & = \Psub{\lambda}{ 0.1 \le \lambda \le 1 | a, b} \\
	& = \Psub{\lambda}{ \lambda \le 1 | a, b} - 
		\Psub{\lambda}{\lambda \le 0.1 | a, b } \\
	& = F_{\lambda | a, b}(1) - F_{\lambda | a, b}(0.1)
\ea 
where $F_{\lambda | a, b}(t)$ is the cdf of a Gamma($a, b$). In particular, since $F$ is a smooth, strictly increasing function, we are warranted in breaking the above equation into two sub-equations, for example,
\ba
0.05 & = \Psub{\lambda}{\lambda \le 0.1} \\
0.05 & = \Psub{\lambda}{\lambda > 1}
\ea
which yields the nonlinear system of equations 
\ba
\begin{pmatrix}
0.05 - F_{\lambda|a, b}(0.1) \\
0.95 - F_{\lambda|a, b}(1) \\
\end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\ea
This system can be easily solved with the \texttt{nleqslv} package in R (the source can be found in Appendix A). Running our code yields
\ba
\boxed{
a \approx 2.44 \htab b \approx 0.184
}
\ea
\newpage
\item
The table of posterior means and their standard deviations is displayed below.
\input{mean_sd.tex}

Comparing to the frequentist analyses performed in the last homework, we see
remarkable agreement.
For reference, the table for the likelihood estimates is given below:
\input{../HW3/mles.tex}
\\
It appears that the standard error of the Bayesian estimates is lower in general than the likelihood-based ones.
Next, we examine histograms of 10,000 samples from the posterior for each length:
\begin{center}
\includegraphics[width=0.9\linewidth]{histograms.pdf}
\end{center}

Next, note that if
 $\lambda | \by \sim \text{Gamma}\left(n + a, b + \sum_{i=1}^n y_i\right)$
we have that
\ba
\left(\frac{1}{\lambda} \right) \bigg| \; \by \sim \text{Inv-Gamma}\left(n + a, 
 	\frac{1}{b + \sum_{i=1}^n y_i} \right).
\ea
However, if we just want histograms of $1/\lambda$ we can simply simulate from our posterior density and then compute the inverse of each sample point. We show these below.
\begin{center}
\includegraphics[width=\linewidth]{inv_histograms.pdf}
\end{center}
\item
First, we note that, for left-probability $l$ and right-probability $r$,
\ba
0.9 & = \Psub{\eta}{ \ell \le \eta \le r }  \\
	& = \Psub{\eta}{ \ell \le \text{LogNormal}(\mu_\eta, \sigma_\eta) \le r } \\
	& = \Psub{\eta}{ \ell \le \exp(\mu_\eta + \sigma_\eta Z) \le r },
		\htab Z \sim \mathcal{N}(0, 1) \\
	& = \Psub{\eta}{ \log(\ell) \le \mu_\eta + \sigma_\eta Z \le \log(r) } \\
	& = \Psub{\eta}{ \frac{\log(l) - \mu_\eta}{\sigma_\eta}
			\le \mathcal{N}(0, 1) \le
			\frac{\log(r) - \mu_\eta}{\sigma_\eta} } \\
	& = \Phi\left( \frac{\log(r) - \mu_\eta}{\sigma_\eta} \right)
		- \Phi\left( \frac{\log(\ell) - \mu_\eta}{\sigma_\eta} \right)
\ea
which we can deconstruct into the set of equations
\ba
\begin{pmatrix}
0.05 - \Phi\left( \frac{\log(\ell) - \mu_\eta}{\sigma_\eta} \right) \\
0.95 - \Phi\left( \frac{\log(r) - \mu_\eta}{\sigma_\eta} \right)
\end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\ea
In particular, for $\eta$, we have
\ba
\begin{pmatrix}
0.05 - \Phi\left( \frac{\log(0.5) - \mu_\eta}{\sigma_\eta} \right) \\
0.95 - \Phi\left( \frac{\log(30) - \mu_\eta}{\sigma_\eta} \right)
\end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}
\ea
and for $\alpha$,
\ba
\begin{pmatrix}
0.05 - \Phi\left( \frac{0 - \mu_\eta}{\sigma_\eta} \right) \\
0.95 - \Phi\left( \frac{\log(4) - \mu_\eta}{\sigma_\eta} \right)
\end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}
\ea
Solving this with \texttt{nleqslv} yields
\ba
\boxed{
\mu_\eta \approx 1.35, \htab \sigma_\eta \approx 1.25
}
\ea
for $\eta$, and 
\ba
\boxed{
\mu_\alpha \approx 0.693, \htab  \sigma_\alpha \approx 0.421
}
\ea
for $\alpha$.
\item
Here we show the tables of medians and (0.05, 0.95) quantiles, the latter of which together construct 95\% credible intervals. 
\\
\input{table1.tex}
\input{table2.tex}
\input{table3.tex}
\input{table4.tex}
\\
Moreover, we include plots of the marginal histograms as well as scatter plots of samples from the joint posterior $p(\eta, \alpha | \by)$, for each of the four lengths in the \texttt{stress} dataset.
\begin{center}
\includegraphics[width=\linewidth]{pairs1.pdf}
\includegraphics[width=\linewidth]{pairs2.pdf}
\includegraphics[width=\linewidth]{pairs3.pdf}
\includegraphics[width=\linewidth]{pairs4.pdf}
\end{center}

Everything looks reasonable and as expected, but it is still imperative to perform  convergence diagnostics and assessment when dealing with MCMC chains. We perform this MCMC analysis for each length in Appendix B. 
\enumend

\newpage
\section*{Problem 2}
\enum
\item
We note that
\ba
\E{ Y_k | X_k } = n \frac{X_k}{N} 
\ea
which suggests the idealized relationship
\ba
 X_k = \frac{N \E{Y_k | X_k}}{n}.
\ea
The most immediate estimate of $\E{Y_k | X_k}$ is simply the observed $Y_k$, yielding
\ba
\estim{X_k} = \frac{N Y_k}{n}
\ea
with variance
\ba
\Var{\estim{X_k}} & = \frac{N^2}{n^2} \Var{Y_k} \\
	& = \frac{N^2}{n^2} \cdot
		n \frac{X_k}{N} \left(1 - \frac{X_k}{N}\right) \frac{N - n}{N - 1} \\
	& = \frac{N(N -n)}{n(N - 1)} X_k \left(1 - \frac{X_k}{N}\right)
\ea
which suggests that
\ba
\estim{\mathsf{Var}}(\estim{X_k}) & = 
	\frac{N^2(N -n)}{n^2(N - 1)} \left(1 - \frac{\frac{N Y_k}{n}}{N}\right) Y_k \\
	& = \frac{N^2(N -n)}{n^2(N - 1)} \left(1 - \frac{Y_k}{n}\right) Y_k \\
\ea
\item
Assume that the likelihood follows the multinomial distribution in (3) and the prior follows a Dirichlet distribution in (4). 
\ba
p(p_1, \dotsc, p_K | x_1, \dotsc, x_K)
	& \propto L(\bx | \bp) \pi(\bp) \\
	& \propto \prod_{i=1}^K p_k^{x_k} \prod_{i=1}^K p_k^{\alpha_k - 1} \\
	& = \prod_{i=1}^K p_k^{x_k + \alpha_k - 1}
\ea
which is exactly the kernel of a Dirichlet with parameters
$x_k + \alpha_k: k = 1, \dotsc, K$.
\item
Here we have
\ba
p(\tilde \bx) & = \int p_{\bX | \bp}(\tilde \bx | \bp)
	\pi(\bp) \; \der \bp \\
	& = \int \frac{N!}{\prod_{k=1}^K x_k!} \prod_{k=1}^K p_k^{x_k}
		\cdot \frac{\Gamma(\alpha_+)}{\prod_{k=1}^K \Gamma(\alpha_k)}
		\prod_{k=1}^K p_k^{\alpha_k - 1}
		\; \der \bp \\
	& = \frac{N! \Gamma(\alpha_+)}{ \prod_k x_k! \prod_k \Gamma(\alpha_k)}
		\int \prod_{k=1}^K p_k^{x_k + \alpha_k - 1} \; \der \bp \\
	& = \frac{N! \Gamma(\alpha_+)}{ \prod_k x_k! \Gamma(\alpha_k)}
		\frac{ \prod_{k=1}^K \Gamma(\alpha_k + x_k)}
			{ \Gamma(( \alpha + x)_+ ) } \\
	& = \frac{N! \Gamma(\alpha_+)}{ \prod_k x_k! \Gamma(\alpha_k)}
		\frac{ \prod_{k=1}^K \Gamma(\alpha_k + x_k)}
			{ \Gamma(\alpha_k + N ) } \\
\ea
which coincides exactly with the CMult($N, \balpha$) as defined in the problem.
\item
We have 
\ba
\E{X_k} & = \Esub{\bp}{ \Esub{\bX | \bp}{ X_k | \bp } } \\
	& = \Esub{\bp}{ N p_k } \htab \text{since } \bX \sim \text{Mult}_k(N, \bp) \\
	& = N \Esub{\bp}{ p_k } \\
	& = \boxed{ \frac{N \alpha_k}{\alpha_+} }
\ea
and
\ba
\Var{X_k} & = \Vsub{\bp}{ \Esub{\bX | \bp}{ X_k | \bp} } +
	\Esub{\bp}{ \Vsub{\bX | \bp}{ X_k | \bp } } \\
	& = \Vsub{\bp}{ N p_k } +
	\Esub{\bp}{ N p_k(1 - p_k) } \\
	& = N^2 \cdot \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) } +
		N \Esub{\bp}{ p_k - p_k^2 } \\
	& = N^2  \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) } +
		N \left[ \frac{\alpha_k}{\alpha_+} - \left( \Vsub{\bp}{p_k} +
			\Esub{\bp}{p_k}^2 \right) \right] \\
	& = N \left\{  N \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) } +
		 \left[ \frac{\alpha_k}{\alpha_+} 
		 -  \frac{ \alpha_k (\alpha_+ - \alpha_k) }
		{\alpha_+^2(\alpha_+ + 1) }
		-	\left( \frac{\alpha_k}{\alpha_+}\right)^2  \right] \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left\{
		\frac{N(\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)}
		+ 1 - \frac{\alpha_+ - \alpha_k}{\alpha_+(\alpha_+ + 1)}
		- \frac{\alpha_k}{\alpha_+} \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left\{
		\frac{(N - 1) (\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)}
		+ 1 - \frac{\alpha_k}{\alpha_+} \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left\{ \frac{1}{1 - \frac{\alpha_k}{\alpha_+}}
		\frac{(N - 1) (\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)} + 1
		 \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left\{ \frac{\alpha_+}{\alpha_+ - \alpha_k}
		\frac{(N - 1) (\alpha_+ - \alpha_k)}{\alpha_+(\alpha_+ + 1)} + 1
		 \right\} \\
	& = \frac{N \alpha_k}{\alpha_+} \left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left\{ 
		\frac{N - 1}{\alpha_+ + 1} + \frac{\alpha_+ + 1}{\alpha_+ + 1}
		 \right\} \\
	& = \boxed{ \frac{N \alpha_k}{\alpha_+} 
		\left( 1 - \frac{\alpha_k}{\alpha_+} \right)
		\left[
		\frac{N + \alpha_+ } {\alpha_+ + 1}
		 \right] }
\ea
\item
Letting $W_k = X_k - y_k$, we see that
\ba
\sum_{k=1}^K W_k = \sum_{k=1}^K X_k - \sum_{k=1}^K y_k
	= N - n
\ea
so that
\ba
W_K & = N - n - \sum_{k=1}^{K - 1} W_k \\
	& = N - n - \sum_{k=1}^{K - 1} (X_k - y_k) 
\ea
Thus,
\ba
W_k = \begin{cases} 
	X_k - y_k, & k < K \\
	N - n - \sum_{k=1}^{K - 1} (X_k - y_k), & k = K
	\end{cases}
\ea
Moreover,
\ba
X_K & = N - \sum_{k=1}^K X_k \\
	& = N - \sum_{k=1}^K (W_k + y_k) \\
\ea
so that
\ba
X_k & = \begin{cases}
	W_k + y_k, & k < K \\
	N - \sum_{k=1}^K (W_k + y_k), & k = K
	\end{cases}.
\ea
Therefore,
\ba
J = \begin{pmatrix}
	1 & 0 & \dotsc & 0 & 0 \\
	0 & 1 & \dotsc & 0 & 0\\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	0 & 0 & \dotsc & 1 & 0 \\
	-1 & -1 & \dotsc & -1 & 1
\end{pmatrix}
\ea
and thus
\ba
\det(J) = \prod_{i=1}^K j_{ii} = 1.
\ea
Hence, noting that $\sum_k y_k = n$,
\ba
f_{W_1, \dotsc, W_k}(w_1, \dotsc, w_K)
	& = f_{X_1, \dotsc, X_K}(w_1 + y_1, \dotsc, w_K + y_K) \cdot \det(J) \\
	& = \frac{N! \Gamma(\alpha_+)}{\Gamma(N + \alpha_+)}
		\prod_{k=1}^K \frac{ \Gamma( w_k + y_k + \alpha_k ) }
			{ (w_k + y_k)! \Gamma(\alpha_k) } \\
	& = \frac{N! \Gamma(\alpha_+)}
		{\Gamma(N + \alpha_+ - n + n)}
		\prod_{k=1}^K \frac{ \Gamma( w_k + y_k + \alpha_k ) }
			{ (w_k + y_k)! \Gamma(\alpha_k) } \cdot 
			 \\
	& = \frac{N! \Gamma(\alpha_+)}
		{\Gamma(N - n + \alpha_+ + \by_+)}
		\prod_{k=1}^K \frac{ \Gamma( w_k + y_k + \alpha_k ) }
			{ (w_k + y_k)! \Gamma(\alpha_k) } \cdot 
			 \\
\ea
\item
Note that
\ba
\Pr{ \bX = \bx | \bY = \by }
	& = \Pr{ \bW + \bY = \bx | \bY = \by } \\
	& = \Pr{ \bW = \bx - \by | \bY = \by }
\ea
which makes sense because the probability of the total underlying counts equal to the probability of the unobserved counts given that we have observed the counts $\bY = \by$. But as shown in (e),
$\bW | \bY = \by \sim \text{CMult}(N - n, \balpha + \by)$.
Thus, from (d), we have
\ba
\Esub{\bX | \bY}{ X_k | \bY = \by}
	& = \Esub{ \bW | \bY }{ W_k + y_k | \bY = \by } \\
	& = \E{ \text{CMult}(N - n, \balpha + \by) } + y_k \\
	& = \boxed{ \frac{(N - n) \left(\alpha_k + y_k \right)}{\alpha_+ + n} + y_k }
\ea
and
\ba
\Vsub{\bX | \bY}{ X_k | \bY = \by}
	& = \Vsub{\bW | \bY}{ W_k + y_k | \bY = \by} \\
	& = \Vsub{\bW | \bY}{ W_k | \bY = \by } \\
	& = \Var{ \text{CMult}(N - n, \balpha + \by) } \\
	& = \frac{(N - n) \left(\alpha_k + y_k \right)}{\alpha_+ + n} 
		\left( 1 - \frac{\alpha_k + y_k }{\alpha_+ + n} \right)
		\left[
		\frac{(N - n) + \alpha_+ + n} {\alpha_+ + n + 1}
		 \right] \\
	& = \boxed{  \frac{(N - n) \left(\alpha_k + y_k \right)}{\alpha_+ + n} 
		\left( 1 - \frac{\alpha_k + y_k }{\alpha_+ + n} \right)
		\left[
		\frac{N + \alpha_+ } {\alpha_+ + n + 1}
		 \right] } \\
\ea
Assuming that $N > n$, i.e., that we haven't sampled the entire population, we see that we will get a positive estimate for both the posterior mean and standard deviation/error, since $\alpha_k + y_k > 0$ and $N + \alpha+ > 0$. Furthermore, if
$\alpha = 0$ for each $k$ (which technically isn't allowed by the model), we get
\ba
\E{X_k | \bY = \by} & = \frac{(N - n) y_k }{ n } + y_k \\
	& = \frac{(N - n)(1 + y_k)}{n}
\ea   
and
\ba
\Var{X_k | \bY = \by} & = 
	 \frac{(N - n) \left( y_k \right)}{ n} 
		\left( 1 - \frac{ y_k }{ n} \right)
		\left[
		\frac{N  } { n + 1}
		 \right]
\ea
which is strictly less than the variance of the method of moments estimator, since, for example, we have $n + 1$ in the denominator rather than $n - 1$, and we are missing a factor of $\frac{N}{n} > 1$. 
\item
Below is the table of the method of moment estimators $\estim{X_k}_\text{,MME}$,
$k = 1, 2, 3$, and their associated standard errors based on the square root of
$\estim{\text{Var}}$ given in (a).
\\
\input{mm_df.tex}
Note that the standard error of $\estim{X_3}$ is zero, which is an artifact of the variance depending on $\estim{X}$ multiplicatively. This is clearly nonsense, as we only have 50/800 of the total population. The source can be found in Appendix C.
\item
Below is the table of posterior means given by $\estim{X_k}_\text{, Bayes}$, along with their empirical standard deviations. We see that for $k = 1, 2$, both estimators yield similar values for both the mean estimate and the standard errors. However, the Bayesian estimate for $X_3$ is positive and has a positive standard error, which can be viewed as an advantage over the method of moments estimator, which is unable to do either. The source can be found in Appendix C.

\input{bayes_df.tex}
\enumend
 
\newpage
\section*{Appendix A: Problem 1 Source}
\lstinputlisting{P1.R}

\section*{Appendix B: MCMC Convergence Analysis}
First, we examine the default plots given by the \texttt{stan\_diag} command, which yields three subplots:
\begin{enumerate}
\item
A histogram of the accumulated log-posterior of the parameters
\item
A histogram of the mean Metropolis acceptance of the parameters
\item
A scatter plot of the values of the first two plots, namely, a scatterplot of log-posterior vs mean. Metropolis acceptance.
\end{enumerate}
We expect that the mean Metropolis acceptance would be higher for values with a higher log-posterior. Indeed, this is the case for each of the four considered lengths, shown below in increasing order of fiber length.
\begin{center}
\includegraphics[width=0.6 \linewidth]{diag1.pdf}
\includegraphics[width=0.5\linewidth]{diag2.pdf}
\includegraphics[width=0.5\linewidth]{diag3.pdf}
\includegraphics[width=0.5\linewidth]{diag4.pdf}
\end{center}
Furthermore, we examine scatter plots of the MCMC chains to ensure their convergence. In particular, we expect the chains to exhibit mostly indistinguishable behavior with respect to the other chains. Indeed, for each parameter and each length, the four chains overlap to a high degree, and there is no visual evidence of non-convergence. Again, the plots are shown in order of increasing fiber length.
\begin{center}
\includegraphics[width=0.8\linewidth]{trace1.pdf}
\includegraphics[width=0.8\linewidth]{trace2.pdf}
\includegraphics[width=0.8\linewidth]{trace3.pdf}
\includegraphics[width=0.8\linewidth]{trace4.pdf}
\end{center}

\newpage
\section*{Appendix C: Problem 2 Source}
\lstinputlisting{P2.R}

\end{document}



























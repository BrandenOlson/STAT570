\documentclass[11pt]{article}
\setlength{\topmargin}{-1in}
\setlength{\textheight}{10in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\textwidth}{7in}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{array}
\usepackage{bm}
\usepackage{cancel}
\usepackage{cite}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{units}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{ mathrsfs }
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{setspace}

\pagenumbering{gobble}

\usepackage{floatrow}
\floatsetup[figure]{capposition=top}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}

\usepackage{color}
\lstset{language=R,basicstyle=\ttfamily,breaklines=true,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{magenta}\ttfamily,
                showstringspaces=false,
                }

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand*\mb{\mathbf}
\newcommand*\reals{\mathbb{R}}
\newcommand*\complex{\mathbb{C}}
\newcommand*\naturals{\mathbb{N}}
\newcommand*\nats{\naturals}
\newcommand*\integers{\mathbb{Z}}
\newcommand*\rationals{\mathbb{Q}}
\newcommand*\irrationals{\mathbb{J}}
\newcommand*\pd{\partial}
\newcommand*\htab{\hspace{4 mm}}
\newcommand*\vtab{\vspace{0.5 in}}
\newcommand*\lsent{\mathcal{L}}
\newcommand*\mean[1]{\overline{#1}}
\newcommand*\union{\cup}
\newcommand*\intersect{\cap}
\newcommand*\cl{\cancel}
\newcommand*\ANS{\text{ANS}}
\newcommand*\As{\text{As}}
\newcommand*\then{\rightarrow}
\newcommand*\elim{\text{E}}
\newcommand*\intro{\text{I}}
\newcommand*\absurd{\curlywedge}
\newcommand*\NK{\vdash_{\text{NK}}}
\newcommand*\interp{\mathcal{I}}
\newcommand*\ba{\[ \begin{aligned}}
\newcommand*\ea{\end{aligned} \]}
\newcommand*\C{\mathcal{C}}
\newcommand*\card[1]{\text{card}\left(#1\right)}
\newcommand*\D{\mathscr{D}}
\newcommand*\df{=_{\text{def}}}
\newcommand*\eps{\epsilon}
\newcommand*\enum{\begin{enumerate}[label=(\alph*)]}
\newcommand*\enumend{\end{enumerate}}
\newcommand*\E[1]{\mathsf{E}\left[#1\right]}
\newcommand*\Esub[2]{\mathsf{E}_{#1}\left[#2\right]}
\newcommand*\Var[1]{\mathsf{Var}\left(#1\right)}
\newcommand*\SD[1]{\mathsf{SD}\left(#1\right)}
\newcommand*\Cov[1]{\;\text{Cov}\left[#1\right]}
\newcommand*\iid{\overset{\text{iid}}{\sim}}
\newcommand*\Exp[1][\lambda]{\text{Exp}(\text{rate}=#1)}
\newcommand*\ind[1]{\mathbf{1}\left(#1\right)}
\newcommand*\set[1]{\left\{#1\right\}}
\newcommand*\estim[1]{\widehat{#1}}
\newcommand*\der{\text{d}}
\newcommand*\deriv[2]{\frac{\der #1}{\der #2}}
\newcommand*\pderiv[2]{\frac{\pd #1}{\pd #2}}
\newcommand*\norm[1]{\left\|#1\right\|}
\newcommand*\dist[2]{\;\text{dist}\left(#1, #2\right)}
\newcommand*\interior{\text{int}\;}
\newcommand*\exterior{\text{ext}\;}
\newcommand*\boundary{\text{bd}\;}
\newcommand*\lh{\overset{\text{L'H}}{=}}
\newcommand*\bA{\mathbf{A}}
\newcommand*\bb{\mathbf{b}}
\newcommand*\bB{\mathbf{B}}
\newcommand*\bc{\mathbf{c}}
\newcommand*\be{\mathbf{e}}
\newcommand*\bh{\mathbf{h}}
\newcommand*\bI{\mathbf{I}}
\newcommand*\bL{\mathbf{L}}
\renewcommand*\bm{\mathbf{m}}
\newcommand*\bo{\mathbf{o}}
\newcommand*\br{\mathbf{r}}
\newcommand*\bs{\mathbf{s}}
\newcommand*\bS{\mathbf{S}}
\newcommand*\bt{\mathbf{t}}
\newcommand*\bu{\mathbf{u}}
\newcommand*\bv{\mathbf{v}}
\newcommand*\bV{\mathbf{V}}
\newcommand*\bx{\mathbf{x}}
\newcommand*\bw{\mathbf{w}}
\newcommand*\bW{\mathbf{W}}
\newcommand*\bX{\mathbf{X}}
\newcommand*\by{\mathbf{y}}
\newcommand*\bY{\mathbf{Y}}
\newcommand*\bZ{\mathbf{Z}}
\newcommand*\bz{\mathbf{z}}
\newcommand*\bp{\mathbf{p}}
\newcommand*\bzero{\mathbf{0}}
\newcommand*\bone{\mathbf{1}}
\newcommand*\balpha{\boldsymbol{\alpha}}
\newcommand*\bbeta{\boldsymbol{\beta}}
\newcommand*\bgamma{\boldsymbol{\gamma}}
\newcommand*\beps{\boldsymbol{\varepsilon}}
\newcommand*\btheta{\boldsymbol{\theta}}
\newcommand*\bTheta{\boldsymbol{\Theta}}
\newcommand*\bmu{\boldsymbol{\mu}}
\newcommand*\bsigma{\boldsymbol{\sigma}}
\newcommand*\bSigma{\boldsymbol{\Sigma}}
\newcommand*\bOmega{\boldsymbol{\Omega}}
\newcommand\Psub[2]{\mathsf{P}_{#1}\left(#2\right)}
\newcommand\Vsub[2]{\mathsf{Var}_{#1}\left(#2\right)}
\newcommand\e{\operatorname{e}}
\newcommand\prox{\operatorname{prox}}
\newcommand\T{\mathsf{T}}

\newread\tmp
\newcommand\getcount{
	\openin\tmp=knot_count.txt
	\read\tmp to \knots
	\closein\tmp
	
	\openin\tmp=span.txt
	\read\tmp to \spanval
	\closein\tmp
	
	\openin\tmp=span_g.txt
	\read\tmp to \spantwo
	\closein\tmp
	
	\openin\tmp=span_g_2.txt
	\read\tmp to \spanthree
	\closein\tmp
}


\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\renewcommand\;{\,}
\renewcommand\epsilon{\varepsilon}
\renewcommand\rho{\varrho}
\renewcommand\phi{\varphi}
\renewcommand\mod{\hspace{0.2em} \textbf{mod}\hspace{0.2em}}
\renewcommand\Pr[1]{ \mathsf{Pr}\left(#1\right) }

\lstset{breaklines=true,
        numbersep=5pt,
        xleftmargin=.25in,
        xrightmargin=.25in}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\vspace{-1in}
\begin{document}
\title{STAT 570: Midterm}
\author{Branden Olson}
\date{}
\maketitle

\section*{Problem 1}
\enum
\item  % Part (a)
    Since a random sample implies that $Y_1, \dotsc, Y_n$ are a i.i.d, we have
\ba
\E{\estim{\mu}} & = 
    \frac{1}{n} \sum_{i=1}^n \E{Y_i} \\
    & = \frac{1}{n} \sum_{i=1}^n \E{Y_1} \htab \text{(by i.i.d)} \\
    & = \frac{ n \cdot \E{Y_1} }{n} \\
    & = \E{Y_1} \qed
\ea
Moreover,
\ba
\Var{\estim{\mu}}
    & = \frac{1}{n^2} \Var{\sum_{i=1}^n Y_i } \\
    & = \frac{1}{n^2} \sum_{i=1}^n \Var{Y_i} \htab \text{(by independence)} \\
    & = \frac{1}{n^2} \sum_{i=1}^n \Var{Y_1} \htab \text{(by i.i.d)} \\
    & = \frac{ n \cdot \Var{Y_1} }{n^2} \\
    & = \frac{\Var{Y_1}}{n}
\ea

\item % Part(b)
    Here, we have
    \ba
    \E{\estim{\mu}_\text{rat}} & = \overline{z_P} 
        \E{\frac{\mean{Y}}{\mean{Z}}} \\
    \ea
which is unbiased only if $\E{\frac{\mean{Y}}{\overline{Z}}} = \frac{\mu}{\overline{z_P}}$, which is not the case in general.
On the other hand, given that $\E{Y} < \infty$ and $\E{Z} < \infty$, by the WLLN, we have both $\overline{Y} \to_P \E{Y}$ and 
$\overline{Z} \to_P \overline{z_P}$, so that by slutsky and continuous mapping,
$\frac{\overline{Y}}{\overline{Z}} \to_P \frac{\E{Y}}{\overline{z_P}}$. 
Thus,
\ba
\estim{\mu}_\text{rat} \to_P \overline{z_P} \frac{ \E{Y} }{\overline{z_P}} 
    = \E{Y}.
\ea
That is, $\estim{\mu}_\text{rat}$ is a consistent estimator.
As for the variance, we can use Taylor approximations as follows.
First, note that
\ba
f(y, z) = f(\mu_Y, \mu_Z) + f_Y(\mu_Y, \mu_Z) (y - \mu_Y) 
	+ f_Z(\mu_Y, \mu_Z)(z - \mu_Z) + \mathcal O(yz)
\ea
which means that
\ba
\E{ f(Y, Z) } & = f(\mu_Y, \mu_Z) + f_Y(\mu_Y, \mu_Z) \E{Y - \mu_Y}
	+ f_Z(\mu_Y, \mu_Z) \E{Z - \mu_Z} + \E{\mathcal O(YZ)} \\
	& = f(\mu_Y, \mu_Z) + 0 + 0 + \E{\mathcal O(YZ) } \\
	& \approx f(\mu_Y, \mu_Z).
\ea
Thus,
\ba
\Var{f(Y, Z)} & \equiv \E{(f(Y, Z) - \E{f(Y, Z)})^2 } \\
	& \approx \E{( f(Y, Z) - f(\mu_Y, \mu_Z) )^2 }.
\ea
Taking another Taylor approximation,
\ba
\Var{f(Y, Z)} & \approx
	\E{ \left( f(\mu_Y, \mu_Z) + f_Y(\mu_Y, \mu_Z) (Y - \mu_Y) 
	+ f_Z(\mu_Y, \mu_Z)(Z - \mu_Z) 
	+ \mathcal O(YZ) - f(\mu_Y, \mu_Z) \right)^2 } \\
	& = \E{ \left( f_Y(\mu_Y, \mu_Z) (Y - \mu_Y) + f_Z(\mu_Y, \mu_Z)(Z - \mu_Z)
		+ \mathcal O(YZ) \right)^2 } \\ 
	& \approx
	f_Y^2(\mu_Y, \mu_Z) \E{(Y - \mu_Y)^2}
	+ 2 f_Y(\mu_Y, \mu_Z) f_Z(\mu_Y, \mu_Z) \E{(Y - \mu_Y)(Z - \mu_Z)} \\
	& \htab + f_Z^2(\mu_Y, \mu_Z) \E{(Z - \mu_Z)^2} \\
	& = f_Y^2(\mu_y, \mu_Z) \Var{Y} 
	+ 2 f_Y(\mu_Y, \mu_Z) f_Z(\mu_Y, \mu_Z) \Cov{Y, Z} 
	+ f_Z^2(\mu_Y, \mu_Z)\Var{Z} \\
\ea
In particular, for $f(\mean{Y}, \mean{Z}) = \frac{\mean{Y}}{\mean{Z}}$, we have
\ba
f_y(y, z) = \frac{1}{z} 
\implies f_y(\mu_Y, \mu_Z) = \frac{1}{\mu_Z} 
\ea
\ba
f_z(y, z) = -\frac{y}{z^2}
\implies f_z(\mu_Y, \mu_Z) = - \frac{\mu_Y}{\mu_Z^2}
\ea
\ba
\Var{\frac{\mean{Y}}{\mean{Z}}} & \approx
	\left[\frac{1}{\mu_Z}\right]^2 \Var{\mean Y}
	+ 2 \frac{1}{\mu_Z} \left(\frac{- \mu_Y}{\mu_Z^2}\right) \Cov{\mean Y, \mean Z}
	+ \left[\frac{-\mu_Y}{\mu_Z^2} \right]^2 \Var{\mean Z} \\
	& = \frac{\sigma_Y^2}{n \mu_Z^2}
		- \frac{ 2 \mu_Y \frac{1}{n^2} \sum_i \sum_j \Cov{Y_i, Z_j} }
			{ \mu_Z^3 }
		+ \frac{ \mu_Y^2 \sigma_Z^2 }{n \mu_Z^4} \\
	& = \frac{1}{n \mu_Z^2} \left[ \sigma_Y^2
		- \frac{2 \mu_Y \Cov{Y_i, Z_i} }{ \mu_Z }
		+ \frac{\mu_Y^2 \sigma_Z^2}{ \mu_Z^2 }
		\right]
\ea
 
\item
    Assuming that the usual linear regression assumptions are met, and that 
    $\E{\estim{\beta_1} | \bZ} = \beta_1$, a constant, we have
    \ba
        \E{\estim{\mu}_\text{reg}}
        & = \E{\overline{Y}} + \E{\estim{\beta_1} 
    \left(\overline{Z} - \overline{z_P}\right)} \\
    & = \mu + \E{\E{ \estim{\beta_1} (\overline{Z} - \overline{z_P}) | \bZ } } \\
    & = \mu + \E{ \left(\overline{Z} - \overline{z_P}\right) \E{ \beta_1 | \bZ } } \\
    & = \mu + \E{\left(\overline{Z} - \overline{z_P}\right) \cdot \beta_1 } \\
    & = \mu + \beta_1 \E{ \overline{Z} - \overline{z_P} } \\
    & = \mu + 0 \\
    & = \mu
    \ea
    Moreover, if $\E{Y} < \infty$ and $\E{Z} < \infty$, by WLLN, $\mean{Y} \to_P \mu$ and $\mean{Z} \to_p \mean{z_P}$. Additionally, under the least squares assumptions, Wald's theorem yields $\estim{\beta_1} \to_p \beta_1$.
    Thus, we have by Slutsky that
\ba
\estim{\mu}_\text{reg} \to \mu + \beta_1 \cdot (\mean{z_P} - \mean{z_P}) = \mu.
\ea
Thus, $\estim{\mu}_\text{reg}$ is consistent.
As for an approximate variance, we have
\ba
\Var{\estim{\mu}_\text{reg}} 
& = \Var{ \mean{Y} + \estim{\beta_1} (\mean{Z} - \mean{z_P} }
\ea
\item 
    Here, we have
    \ba
        \E{\estim{\mu}} 
        & = \frac{1}{n} \sum_{i=1}^n \E{Y_i} \\
        & = \frac{1}{n} \left[
            \sum_{i: X_i = 0} \E{Y_i} + \sum_{i: X_i = 1} \E{Y_i} \right] \\
        & = \frac{1}{n} \left[
            n_0 \cdot \mu_0 + n_1 \cdot \mu_1
        \right] \\
        & = \boxed{ \frac{ n_0 \mu_0 + n_1 \mu_1 }{n} }
    \ea
    whereas
    \ba
        \Var{\estim{\mu}}
        & = \frac{1}{n^2} \sum_{i=1}^n \Var{Y_i} \htab \text{(by independence)} \\
        & = \frac{1}{n^2} \left[
            \sum_{i:X_i = 0} \Var{Y_i} + \sum_{i:X_i = 1} \Var{Y_i}
        \right] \\
        & = \boxed{ \frac{ n_0 \sigma_0^2 + n_1 \sigma_1^2 }{n^2} }
    \ea
\item
    The estimator $\estim{\mu}$ solves $G(\estim\mu) = 0$, i.e. 
    \ba
    0 = G(\estim\mu) = \sum_{i=1}^n w_i (Y_i - \estim\mu) = \sum_{i=1}^n w_i Y_i - \estim\mu \sum_{i=1}^n w_i \\
    \implies \estim\mu \sum_{i=1}^n w_i = \sum_{i=1}^n w_i Y_i \\
\implies \estim\mu = \frac{\sum_{i=1}^n w_i Y_i}{\sum_{i=1}^n w_i} \qed
    \ea
    In our case,
    \ba
        w_i = \frac{1}{\pi_i} 
        & = \begin{cases}
        \frac{1}{N_0/N}, & X_i = 0 \\
        \frac{1}{N_1/N}, & X_i = 1
    \end{cases} 
    = \begin{cases}
        \frac{N}{N_0}, & X_i = 0 \\
        \frac{N}{N_1}, & X_i = 1
    \end{cases}
    \ea
    yielding
    \ba
        \E{\estim{\mu}_w} & = \frac{1}{\sum_{i=1}^n w_i} \sum_{i=1}^n w_i \E{Y_i} \\
        & = \frac{1}{n_0 \cdot \frac{N}{N_0} + n_1 \cdot \frac{N}{N_1}}
            \left[ \sum_{i:X_i=0} w_i \E{Y_i} + \sum_{i:X_i=1} w_i \E{Y_i} \right] \\
        & = \frac{1}{\frac{n_0 N_1 N + n_1 N_0 N}{N_0 N_1}}
            \left[ n_0 \frac{N}{N_0} \mu_0 + n_1 \frac{N}{N_1} \mu_1 \right] \\
        & = \frac{ n_0 N_1 \mu_0 + n_1 N_0 \mu_1}{ n_0 N_1 + n_1 N_0 }
    \ea
    and
    \ba
        \Var{\estim{\mu}_w}
        & = \frac{1}{\left(\sum_i w_i\right)^2}
            \sum_{i=1}^n w_i^2 \Var{Y_i} \\
            & = \frac{1}{ \left(\frac{n_0 N_1 N + n_1 N_0 N}{N_0 N_1} \right)^2 }
            \left[ \sum_{i:X_i=0} w_i^2 \Var{Y_i}
                    + \sum_{i:X_i=1} w_i^2 \Var{Y_i}
                \right] \\
        & = \frac{ N_0^2 N_1^2 \left[ n_0 \left(\frac{N}{N_0}\right)^2 \sigma_0^2
        + n_1 \left(\frac{N}{N_1}\right)^2 \sigma_1^2 \right]}
        { (n_0 N_1 + n_1 N_0)^2 N^2 } \\
        & = \frac{ n_0 N_1^2 \sigma_0^2 + n_1 N_0^2 \sigma_1^2 }
            { (n_0 N_1 + n_1 N_0)^2}
    \ea
\enumend

\section*{Appendix A: Problem 1 Source}
\lstinputlisting{P1.R}
\end{document}


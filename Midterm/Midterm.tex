\documentclass[11pt]{article}
\setlength{\topmargin}{-1in}
\setlength{\textheight}{10in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\textwidth}{7in}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{array}
\usepackage{bm}
\usepackage{cancel}
\usepackage{cite}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{units}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{ mathrsfs }
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{setspace}

\pagenumbering{gobble}

\usepackage{floatrow}
\floatsetup[figure]{capposition=top}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}

\usepackage{color}
\lstset{language=R,basicstyle=\ttfamily,breaklines=true,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{magenta}\ttfamily,
                showstringspaces=false,
                }

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand*\mb{\mathbf}
\newcommand*\reals{\mathbb{R}}
\newcommand*\complex{\mathbb{C}}
\newcommand*\naturals{\mathbb{N}}
\newcommand*\nats{\naturals}
\newcommand*\integers{\mathbb{Z}}
\newcommand*\rationals{\mathbb{Q}}
\newcommand*\irrationals{\mathbb{J}}
\newcommand*\pd{\partial}
\newcommand*\htab{\hspace{4 mm}}
\newcommand*\vtab{\vspace{0.5 in}}
\newcommand*\lsent{\mathcal{L}}
\newcommand*\mean[1]{\overline{#1}}
\newcommand*\union{\cup}
\newcommand*\intersect{\cap}
\newcommand*\cl{\cancel}
\newcommand*\ANS{\text{ANS}}
\newcommand*\As{\text{As}}
\newcommand*\then{\rightarrow}
\newcommand*\elim{\text{E}}
\newcommand*\intro{\text{I}}
\newcommand*\absurd{\curlywedge}
\newcommand*\NK{\vdash_{\text{NK}}}
\newcommand*\interp{\mathcal{I}}
\newcommand*\ba{\[ \begin{aligned}}
\newcommand*\ea{\end{aligned} \]}
\newcommand*\C{\mathcal{C}}
\newcommand*\card[1]{\text{card}\left(#1\right)}
\newcommand*\D{\mathscr{D}}
\newcommand*\df{=_{\text{def}}}
\newcommand*\eps{\epsilon}
\newcommand*\enum{\begin{enumerate}[label=(\alph*)]}
\newcommand*\enumend{\end{enumerate}}
\newcommand*\E[1]{\;\mathsf{E}\left[#1\right]}
\newcommand*\Esub[2]{\mathsf{E}_{#1}\left[#2\right]}
\newcommand*\Var[1]{\;\mathsf{Var}\left(#1\right)}
\newcommand*\SD[1]{\;\mathsf{SD}\left(#1\right)}
\newcommand*\SE[1]{\;\mathsf{SE}\left(#1\right)}
\newcommand*\Cov[1]{\;\mathsf{Cov}\left(#1\right)}
\newcommand*\iid{\overset{\text{iid}}{\sim}}
\newcommand*\Exp[1][\lambda]{\text{Exp}(\text{rate}=#1)}
\newcommand*\ind[1]{\mathbf{1}\left(#1\right)}
\newcommand*\set[1]{\left\{#1\right\}}
\newcommand*\estim[1]{\widehat{#1}}
\newcommand*\der{\text{d}}
\newcommand*\deriv[2]{\frac{\der #1}{\der #2}}
\newcommand*\pderiv[2]{\frac{\pd #1}{\pd #2}}
\newcommand*\norm[1]{\left\|#1\right\|}
\newcommand*\dist[2]{\;\text{dist}\left(#1, #2\right)}
\newcommand*\interior{\text{int}\;}
\newcommand*\exterior{\text{ext}\;}
\newcommand*\boundary{\text{bd}\;}
\newcommand*\lh{\overset{\text{L'H}}{=}}
\newcommand*\bA{\mathbf{A}}
\newcommand*\bb{\mathbf{b}}
\newcommand*\bB{\mathbf{B}}
\newcommand*\bc{\mathbf{c}}
\newcommand*\be{\mathbf{e}}
\newcommand*\bG{\mathbf{G}}
\newcommand*\bh{\mathbf{h}}
\newcommand*\bI{\mathbf{I}}
\newcommand*\bL{\mathbf{L}}
\renewcommand*\bm{\mathbf{m}}
\newcommand*\bo{\mathbf{o}}
\newcommand*\br{\mathbf{r}}
\newcommand*\bs{\mathbf{s}}
\newcommand*\bS{\mathbf{S}}
\newcommand*\bt{\mathbf{t}}
\newcommand*\bu{\mathbf{u}}
\newcommand*\bv{\mathbf{v}}
\newcommand*\bV{\mathbf{V}}
\newcommand*\bx{\mathbf{x}}
\newcommand*\bw{\mathbf{w}}
\newcommand*\bW{\mathbf{W}}
\newcommand*\bX{\mathbf{X}}
\newcommand*\by{\mathbf{y}}
\newcommand*\bY{\mathbf{Y}}
\newcommand*\bZ{\mathbf{Z}}
\newcommand*\bz{\mathbf{z}}
\newcommand*\bp{\mathbf{p}}
\newcommand*\bzero{\mathbf{0}}
\newcommand*\bone{\mathbf{1}}
\newcommand*\balpha{\boldsymbol{\alpha}}
\newcommand*\bbeta{\boldsymbol{\beta}}
\newcommand*\bgamma{\boldsymbol{\gamma}}
\newcommand*\beps{\boldsymbol{\varepsilon}}
\newcommand*\btheta{\boldsymbol{\theta}}
\newcommand*\bTheta{\boldsymbol{\Theta}}
\newcommand*\bmu{\boldsymbol{\mu}}
\newcommand*\bsigma{\boldsymbol{\sigma}}
\newcommand*\bSigma{\boldsymbol{\Sigma}}
\newcommand*\bOmega{\boldsymbol{\Omega}}
\newcommand\Psub[2]{\mathsf{P}_{#1}\left(#2\right)}
\newcommand\Vsub[2]{\mathsf{Var}_{#1}\left(#2\right)}
\newcommand\e{\operatorname{e}}
\newcommand\prox{\operatorname{prox}}
\newcommand\T{\mathsf{T}}

\newread\tmp
\newcommand\getcount{
	\openin\tmp=knot_count.txt
	\read\tmp to \knots
	\closein\tmp
	
	\openin\tmp=span.txt
	\read\tmp to \spanval
	\closein\tmp
	
	\openin\tmp=span_g.txt
	\read\tmp to \spantwo
	\closein\tmp
	
	\openin\tmp=span_g_2.txt
	\read\tmp to \spanthree
	\closein\tmp
}


\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\renewcommand\;{\,}
\renewcommand\epsilon{\varepsilon}
\renewcommand\rho{\varrho}
\renewcommand\phi{\varphi}
\renewcommand\mod{\hspace{0.2em} \textbf{mod}\hspace{0.2em}}
\renewcommand\Pr[1]{ \mathsf{Pr}\left(#1\right) }

\lstset{breaklines=true,
        numbersep=5pt,
        xleftmargin=.25in,
        xrightmargin=.25in}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\vspace{-1in}
\begin{document}
\title{STAT 570 Midterm}
\author{Branden Olson}
\date{}
\maketitle

\section*{Problem 1}
\enum
\item  % Part (a)
    Since a random sample implies that $Y_1, \dotsc, Y_n$ are a i.i.d, we have
\ba
\E{\estim{\mu}} & = 
    \frac{1}{n} \sum_{i=1}^n \E{Y_i} \\
    & = \frac{1}{n} \sum_{i=1}^n \E{Y_1} \htab \text{(by i.i.d)} \\
    & = \frac{ n \cdot \E{Y_1} }{n} \\
    & = \E{Y_1} \qed
\ea
Moreover,
\ba
\Var{\estim{\mu}}
    & = \frac{1}{n^2} \Var{\sum_{i=1}^n Y_i } \\
    & = \frac{1}{n^2} \sum_{i=1}^n \Var{Y_i} \htab \text{(by independence)} \\
    & = \frac{1}{n^2} \sum_{i=1}^n \Var{Y_1} \htab \text{(by i.i.d)} \\
    & = \frac{ n \cdot \Var{Y_1} }{n^2} \\
    & = \frac{\Var{Y_1}}{n}
\ea

\item % Part(b)
    Here, we have
    \ba
    \E{\estim{\mu}_\text{rat}} & = \overline{z_P} 
        \E{\frac{\mean{Y}}{\mean{Z}}} \\
    \ea
which is unbiased only if $\E{\frac{\mean{Y}}{\overline{Z}}} = \frac{\mu}{\overline{z_P}}$, which is not the case in general.
On the other hand, given that $\E{Y} < \infty$ and $\E{Z} < \infty$, by the WLLN, we have both $\overline{Y} \to_P \E{Y}$ and 
$\overline{Z} \to_P \overline{z_P}$, so that by slutsky and continuous mapping,
$\frac{\overline{Y}}{\overline{Z}} \to_P \frac{\E{Y}}{\overline{z_P}}$. 
Thus,
\ba
\estim{\mu}_\text{rat} \to_P \overline{z_P} \frac{ \E{Y} }{\overline{z_P}} 
    = \E{Y}.
\ea
That is, $\estim{\mu}_\text{rat}$ is a consistent estimator.
As for the variance, we can use Taylor approximations as follows.
First, note that
\ba
f(y, z) = f(\mu_Y, \mu_Z) + f_Y(\mu_Y, \mu_Z) (y - \mu_Y) 
	+ f_Z(\mu_Y, \mu_Z)(z - \mu_Z) + \mathcal O(yz)
\ea
which means that
\ba
\E{ f(Y, Z) } & = f(\mu_Y, \mu_Z) + f_Y(\mu_Y, \mu_Z) \E{Y - \mu_Y}
	+ f_Z(\mu_Y, \mu_Z) \E{Z - \mu_Z} + \E{\mathcal O(YZ)} \\
	& = f(\mu_Y, \mu_Z) + 0 + 0 + \E{\mathcal O(YZ) } \\
	& \approx f(\mu_Y, \mu_Z).
\ea
Thus,
\ba
\Var{f(Y, Z)} & \equiv \E{(f(Y, Z) - \E{f(Y, Z)})^2 } \\
	& \approx \E{( f(Y, Z) - f(\mu_Y, \mu_Z) )^2 }.
\ea
Taking another Taylor approximation,
\ba
\Var{f(Y, Z)} & \approx
	\E{ \left( f(\mu_Y, \mu_Z) + f_Y(\mu_Y, \mu_Z) (Y - \mu_Y) 
	+ f_Z(\mu_Y, \mu_Z)(Z - \mu_Z) 
	+ \mathcal O(YZ) - f(\mu_Y, \mu_Z) \right)^2 } \\
	& = \E{ \left( f_Y(\mu_Y, \mu_Z) (Y - \mu_Y) + f_Z(\mu_Y, \mu_Z)(Z - \mu_Z)
		+ \mathcal O(YZ) \right)^2 } \\ 
	& \approx
	f_Y^2(\mu_Y, \mu_Z) \E{(Y - \mu_Y)^2}
	+ 2 f_Y(\mu_Y, \mu_Z) f_Z(\mu_Y, \mu_Z) \E{(Y - \mu_Y)(Z - \mu_Z)} \\
	& \htab + f_Z^2(\mu_Y, \mu_Z) \E{(Z - \mu_Z)^2} \\
	& = f_Y^2(\mu_y, \mu_Z) \Var{Y} 
	+ 2 f_Y(\mu_Y, \mu_Z) f_Z(\mu_Y, \mu_Z) \Cov{Y, Z} 
	+ f_Z^2(\mu_Y, \mu_Z)\Var{Z} \\
\ea
In particular, for $f(\mean{Y}, \mean{Z}) = \frac{\mean{Y}}{\mean{Z}}$, we have
\ba
f_y(y, z) = \frac{1}{z} 
\implies f_y(\mu_Y, \mu_Z) = \frac{1}{\mu_Z} 
\ea
\ba
f_z(y, z) = -\frac{y}{z^2}
\implies f_z(\mu_Y, \mu_Z) = - \frac{\mu_Y}{\mu_Z^2}
\ea
\ba
\Var{\frac{\mean{Y}}{\mean{Z}}} & \approx
	\left[\frac{1}{\mu_Z}\right]^2 \Var{\mean Y}
	+ 2 \frac{1}{\mu_Z} \left(\frac{- \mu_Y}{\mu_Z^2}\right) \Cov{\mean Y, \mean Z}
	+ \left[\frac{-\mu_Y}{\mu_Z^2} \right]^2 \Var{\mean Z} \\
	& = \frac{\sigma_Y^2}{n \mu_Z^2}
		- \frac{ 2 \mu_Y \frac{1}{n^2} \sum_i \sum_j \Cov{Y_i, Z_j} }
			{ \mu_Z^3 }
		+ \frac{ \mu_Y^2 \sigma_Z^2 }{n \mu_Z^4} \\
	& = \frac{1}{n \mu_Z^2} \left[ \sigma_Y^2
		- \frac{2 \mu_Y \rho_{Y, Z} }{ \mu_Z }
		+ \frac{\mu_Y^2 \sigma_Z^2}{ \mu_Z^2 }
		\right]
\ea
Plugging in the usual estimates $\estim{\mu}, \estim{\sigma^2}, \estim{\rho}$ yields the approximate variance
\ba
\boxed{
\Var{\estim{\mu}_\text{rat}}
	\approx \frac{ \mean{z_P}^2 }{n \estim{\mu_Z}^2}
	\left[ \estim{\sigma_Y^2}
		- \frac{2 \estim{\mu_Y} \estim{\rho_{Y, Z}} }
		{ \estim{\mu_Z} }
		+ \frac{\estim{\mu_Y}^2 \estim{\sigma_Z^2}}{ \estim{\mu_Z}^2 }
		\right]
		}
\ea
I don't think this is a very sensible estimator since it does not convey intuition behind the relationship of $Y$ and $Z$. In essence, it scales the sample mean of $Y$ by the "relative accuracy" of $\mean{Z}$ with respect to its population mean 
$\mean{z_P}$, which to me isn't obviously beneficial.
 
\item
    Assuming that the usual linear regression assumptions are met, and that 
    $\E{\estim{\beta_1} | \bZ} = \beta_1$, a constant, we have
    \ba
        \E{\estim{\mu}_\text{reg}}
        & = \E{\overline{Y}} + \E{\estim{\beta_1} 
    \left(\overline{z_P} - \overline{Z}\right)} \\
    & = \mu + \E{\E{ \estim{\beta_1} 
    	(\overline{z_P} - \overline{Z}) | \bZ } } \\
    & = \mu + \E{ \left(\overline{z_P} 
    	- \overline{Z}\right) \E{ \beta_1 | \bZ } } \\
    & = \mu + \E{\left(\overline{z_P} - \overline{Z}\right) \cdot \beta_1 } \\
    & = \mu + \beta_1 \E{ \overline{z_P} - \overline{Z} } \\
    & = \mu + 0 \\
    & = \mu
    \ea
    Moreover, if $\E{Y} < \infty$ and $\E{Z} < \infty$, by WLLN, $\mean{Y} \to_P \mu$ and $\mean{Z} \to_p \mean{z_P}$. Additionally, under the least squares assumptions, Wald's theorem yields $\estim{\beta_1} \to_p \beta_1$.
    Thus, we have by Slutsky that
\ba
\estim{\mu}_\text{reg} \to \mu + \beta_1 \cdot (\mean{z_P} - \mean{z_P}) = \mu.
\ea
Thus, $\estim{\mu}_\text{reg}$ is consistent.
As for an approximate variance, we can use Taylor approximations again.
Using that $f(x, y, z) \approx f(\mu_x, \mu_y, \mu_z)$ when expanding about 
$\bmu = (\mu_x, \mu_y, \mu_z)$, and similar steps as in (b), we have
\ba
\Var{f(X, Y, Z)} 
	& \approx \E{ \left(f(X, Y, Z) - f(\bmu)\right)^2 } \\
	& \approx \E{ \left( f_x(\bmu)(X - \mu_X) + f_y(\bmu)(Y - \mu_Y)
		+ f_z(\bmu)(Z - \mu_Z) \right)^2 } \\
	& = f_x^2(\bmu) \Var{X} + 2 (f_x f_y)(\bmu) \Cov{X, Y}
		+ 2(f_x f_z)(\bmu) \Cov{X, Z} + f_y^2(\bmu) \Var{Y} \\
		& \htab + 2 (f_y f_z)(\bmu) \Cov{Y, Z} + f_z^2(\bmu) \Var{Z}
\ea
Hence, with $\mean Y + \estim{\beta_1}(\mean{z_P} - \mean{Z}) 
= f(\mean Y, \estim{\beta_1}, \mean{z_P} - \mean{Z}) \equiv f(X, Y, Z) = X + YZ$, we have $\bmu = \left(\mu_Y, \beta_1, 0\right)$, and
\ba
f_x(X, Y, Z) \equiv 1 \implies f_x(\bmu) \equiv 1 \\
f_y(X, Y, Z) = Z  \implies f_y(\bmu) = 0 \\
f_z(X, Y, Z) = Y \implies f_z(\bmu) = \beta_1.
\ea
Therefore,
\ba
\Var{\mean{Y} + \estim{\beta_1} \left(\mean{z_P} - \mean{Z}\right) }
	& \approx \Var{\mean{Y}} + 2 \cdot 1 \cdot \beta_1 \Cov{\mean{Y}, -\mean{Z}} 
	+ \beta_1^2 \Var{\mean{Z}} \\
	& = \frac{\sigma_Y^2}{n} - 2 \beta_1 \frac{\Cov{Y, Z}}{n}
		+ \beta_1^2 \frac{\sigma_Z^2}{n} \\
	& = \frac{1}{n} \left[ \sigma_Y^2 - 2 \beta_1 \rho_{Y, Z}
	 + \beta_1^2 \sigma_Z^2 \right] \\
	& \approx \boxed{ \frac{1}{n} \left[ \estim{\sigma_Y^2} - 2 \estim{\beta_1}
		\estim{ \rho_{Y, Z} }
	 + \estim{\beta_1}^2 \estim{\sigma_Z^2} \right] } \\
\ea
I believe this is a more sensible estimator since it is unbiased, consistent, and relates $\mean{Y}$ and $\mean{Z}$ in a more natural linear way as opposed to a ratio. Furthermore, its form is similar to a linearization, in that we can write
\ba
\estim{\mu}_\text{reg} - \mean{Y} = \estim{\beta_1} ( \mean{z_P} - \mean{Z} ),
\ea
in point-slope form.
\item 
    Here, we have
    \ba
        \E{\estim{\mu}} 
        & = \frac{1}{n} \sum_{i=1}^n \E{Y_i} \\
        & = \frac{1}{n} \left[
            \sum_{i: X_i = 0} \E{Y_i} + \sum_{i: X_i = 1} \E{Y_i} \right] \\
        & = \frac{1}{n} \left[
            n_0 \cdot \mu_0 + n_1 \cdot \mu_1
        \right] \\
        & = \boxed{ \frac{ n_0 \mu_0 + n_1 \mu_1 }{n} }
    \ea
    whereas
    \ba
        \Var{\estim{\mu}}
        & = \frac{1}{n^2} \sum_{i=1}^n \Var{Y_i} \htab \text{(by independence)} \\
        & = \frac{1}{n^2} \left[
            \sum_{i:X_i = 0} \Var{Y_i} + \sum_{i:X_i = 1} \Var{Y_i}
        \right] \\
        & = \boxed{ \frac{ n_0 \sigma_0^2 + n_1 \sigma_1^2 }{n^2} }
    \ea
\item
    The estimator $\estim{\mu}$ solves $G(\estim\mu) = 0$, i.e. 
    \ba
    0 = G(\estim\mu) = \sum_{i=1}^n w_i (Y_i - \estim\mu) = \sum_{i=1}^n w_i Y_i - \estim\mu \sum_{i=1}^n w_i \\
    \implies \estim\mu \sum_{i=1}^n w_i = \sum_{i=1}^n w_i Y_i \\
\implies \estim\mu = \frac{\sum_{i=1}^n w_i Y_i}{\sum_{i=1}^n w_i} \qed
    \ea
    In our case,
    \ba
        w_i = \frac{1}{\pi_i} 
        & = \begin{cases}
        \frac{1}{N_0/N}, & X_i = 0 \\
        \frac{1}{N_1/N}, & X_i = 1
    \end{cases} 
    = \begin{cases}
        \frac{N}{N_0}, & X_i = 0 \\
        \frac{N}{N_1}, & X_i = 1
    \end{cases}
    \ea
    yielding
    \ba
        \E{\estim{\mu}_w} & = \frac{1}{\sum_{i=1}^n w_i} \sum_{i=1}^n w_i \E{Y_i} \\
        & = \frac{1}{n_0 \cdot \frac{N}{N_0} + n_1 \cdot \frac{N}{N_1}}
            \left[ \sum_{i:X_i=0} w_i \E{Y_i} + \sum_{i:X_i=1} w_i \E{Y_i} \right] \\
        & = \frac{1}{\frac{n_0 N_1 N + n_1 N_0 N}{N_0 N_1}}
            \left[ n_0 \frac{N}{N_0} \mu_0 + n_1 \frac{N}{N_1} \mu_1 \right] \\
        & = \frac{ n_0 N_1 \mu_0 + n_1 N_0 \mu_1}{ n_0 N_1 + n_1 N_0 }
    \ea
    and
    \ba
        \Var{\estim{\mu}_w}
        & = \frac{1}{\left(\sum_i w_i\right)^2}
            \sum_{i=1}^n w_i^2 \Var{Y_i} \\
            & = \frac{1}{ \left(\frac{n_0 N_1 N + n_1 N_0 N}{N_0 N_1} \right)^2 }
            \left[ \sum_{i:X_i=0} w_i^2 \Var{Y_i}
                    + \sum_{i:X_i=1} w_i^2 \Var{Y_i}
                \right] \\
        & = \frac{ N_0^2 N_1^2 \left[ n_0 \left(\frac{N}{N_0}\right)^2 \sigma_0^2
        + n_1 \left(\frac{N}{N_1}\right)^2 \sigma_1^2 \right]}
        { (n_0 N_1 + n_1 N_0)^2 N^2 } \\
        & = \frac{ n_0 N_1^2 \sigma_0^2 + n_1 N_0^2 \sigma_1^2 }
            { (n_0 N_1 + n_1 N_0)^2}
    \ea
\item
The source code for the rest of this problem can be found in Appendix A. We show a table of the estimated means and standard errors for each estimator below.
\input{mus.tex} \\
There seems to be high agreement between both the means and standard errors, with $\estim{\mu}_\text{reg}$ exhibiting the lowest SE. This is perhaps my preferred estimator, as it is has the tightest uncertainty bounds, has a nice linear form, and attempts to include information about $\bZ$ without resorting to ratios (which can be more unstable for smaller sample sizes).
\item
Below we include a table of the estimated means and standard errors for each estimator below.
\input{mu2.tex}
\enumend

\section*{Problem 2}
The source code for this problem can be found in Appendix B.
\enum
\item
Using \texttt{lm} in R, we find that
\ba
\estim{\beta_1} & \approx 0.04596 \\
\SE{\estim{\beta_1}} & \approx 0.004991
\ea
\item
Again using \texttt{lm}, we find that
\ba
\estim{\beta_1} & \approx 0.02803 \\
\SE{\estim{\beta_1}} & \approx 0.006510
\ea
However, this is not an appropriate analysis if the modeler intends to understand the differences of SBP between strata. A quick look at a plot of the data along with the fitted line from the above slope paints the picture:
\begin{center}
\includegraphics[width=\linewidth]{fit_b.pdf}
\end{center}
It is clear that we are fitting a line between two different clusters, instead of fitting two separate lines as we should, unless we don't care about differences between strata. Including an interaction between birthweight and race would do the trick.
\item
Including race as a covariate, our values become
\ba
\estim{\beta_1} & \approx 0.02988 \\
\SE{\estim{\beta_1}} & \approx 0.0006527
\ea
Note that this model takes the form
\ba
\E{\text{SBP}_i} & = \beta_0 + \beta_1 \text{BirthWeight}_i + \beta_2 \text{Race}_i \\
	& = \begin{cases}
		\beta_0 + \beta_1 \text{BirthWeight}_i, \htab \text{Race}_i = 0 \\
		\beta_0 + \beta_1 \text{BirthWeight}_i + \beta_2 \cdot 1, \htab \text{Race}_i = 1
		\end{cases} \\
	& = \begin{cases}
		\beta_0 + \beta_1 \text{BirthWeight}_i, \htab \text{Race $i$ is African American} \\
		\left(\beta_0 + \beta_2 \right) + \beta_1 \text{BirthWeight}_i, \htab \text{Race $i$ is European American}
	\end{cases}
\ea
Thus, AA and EA individuals are assumed to have different intercepts but the same slope. This allows for the interpretation of $\beta_1$ as the associated response in a unit change in SBP regardless of race, but given that the race intercepts have been separately inferred.
\item
Including sample weights, our values become
\ba
\estim{\beta_1} & \approx 0.02988 \\
\SE{\estim{\beta_1}} & \approx 0.0006527
\ea
This model takes the form
\ba
\E{\text{SBP}_i} & = \beta_0 + \beta_1 \text{BirthWeight}_i 
	+ \beta_2 w_i \\
	& = \begin{cases}
		\beta_0 + \beta_1 \text{BirthWeight}_i + \beta_2 \frac{N}{N_0},
		 \htab \text{Race}_i = 0 \\
		\beta_0 + \beta_1 \text{BirthWeight}_i 
		+ \beta_2 \frac{N}{N_1}, \htab \text{Race}_i = 1
		\end{cases} \\
	& = \begin{cases}
		\left(\beta_0 + \frac{\beta_2 N}{N_0} \right)
		+ \beta_1 \text{BirthWeight}_i, 
		\htab \text{Race $i$ is African American} \\
		\left(\beta_0 + \frac{\beta_2 N}{N_1} \right) 
		+ \beta_1 \text{BirthWeight}_i,
		 \htab \text{Race $i$ is European American}
	\end{cases}
\ea
The slope $\beta_1$ represents the associated response in a unit change in SBP regardless of race, given that the intercepts are regressed on the design weights with a common intercept. 
\item
The estimating equation estimator satisfies
\ba
0 = \bG(\bbeta) = \bx^T \bw (\bY - \bx \bbeta) 
	= \bx^T \bw \bY  - \bx^T \bw \bx \bbeta
\ea
so that, assuming $\bx^T \bw \bx \in \reals^{2 \times 2}$ is invertible, we have
\ba
\implies \bx^T \bw \bx \estim\bbeta_w = \bx^T \bw \bY \\
\implies \estim\bbeta_w = \left(\bx^T \bw \bx\right)^{-1} \bx^T \bw \bY
\ea
Furthermore, we have
\ba
\Var{\estim{\beta}_w} 
& = \Var{\left(\bx^T \bw \bx\right)^{-1} \bx^T \bw \bY } \\
& = \left(\bx^T \bw \bx\right)^{-1} \bx^T \bw \Var{\bY}
	\left[ \left(\bx^T \bw \bx\right)^{-1} \bx^T \bw \right]^T \\
& = \sigma_Y^2 \left(\bx^T \bw \bx\right)^{-1} \bx^T \bw 
	\bw^T \bx \left(\bx^T \bw \bx\right)^{-T}
\ea
These expressions yield the estimates
\ba
\estim{\beta}_w & \approx 0.01673 \\
\SE{\estim{\beta}_w} & \approx 0.008831
\ea
\item
In conclusion, it seems that including birthweight, race, and their interaction, would be the most sensible way to model the SBP data. This matches our intuition that each stratum will possibly have its own trend, i.e. its own slope and intercept, in general. 
\enumend

\tiny
\section*{Appendix A: Problem 1 Source}
\lstinputlisting{P1.R}

\section*{Appendix B: Problem 2 Source}
\lstinputlisting{P2.R}
\end{document}

